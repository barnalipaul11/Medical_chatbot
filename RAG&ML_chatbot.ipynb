{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "49_C8lwlQeUn",
        "outputId": "35d807f0-8b56-4775-96d4-ca7055b73a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.24.6)\n",
            "Collecting langchain-core<0.4,>=0.3.0 (from langchain-huggingface)\n",
            "  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
            "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.117 (from langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading langsmith-0.1.120-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.9.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (9.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.120-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, sentence-transformers, langchain-core, langchain-huggingface\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.0 langchain-huggingface-0.1.0 langsmith-0.1.120 orjson-3.10.7 sentence-transformers-3.1.0 tenacity-8.5.0\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 watchdog-4.0.2\n",
            "Collecting pypdf (from -r requirements.txt (line 1))\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting langchain (from -r requirements.txt (line 2))\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.4.0+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.34.2)\n",
            "Collecting bitsandbytes (from -r requirements.txt (line 5))\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting ctransformers (from -r requirements.txt (line 6))\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.1.0)\n",
            "Collecting faiss_cpu (from -r requirements.txt (line 8))\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting chainlit (from -r requirements.txt (line 9))\n",
            "  Downloading chainlit-1.1.404-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.24.6)\n",
            "Collecting langchain_community (from -r requirements.txt (line 11))\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (0.3.0)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (0.1.120)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (0.4.5)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers->-r requirements.txt (line 6)) (9.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 7)) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 7)) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 7)) (9.4.0)\n",
            "Collecting aiofiles<24.0.0,>=23.1.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting asyncer<0.0.8,>=0.0.7 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading asyncer-0.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 9)) (8.1.7)\n",
            "Collecting dataclasses_json<0.6.0,>=0.5.7 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting fastapi<0.111.0,>=0.110.1 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 9)) (0.27.2)\n",
            "Collecting lazify<0.5.0,>=0.4.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading Lazify-0.4.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting literalai==0.0.607 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading literalai-0.0.607.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 9)) (1.6.0)\n",
            "Collecting packaging>=20.0 (from accelerate->-r requirements.txt (line 4))\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 9)) (2.9.0)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting python-multipart<0.0.10,>=0.0.9 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting python-socketio<6.0.0,>=5.11.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading python_socketio-5.11.4-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting syncer<3.0.0,>=2.0.3 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading syncer-2.0.3.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 9)) (2.0.1)\n",
            "Collecting uptrace<2.0.0,>=1.22.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading uptrace-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting uvicorn<0.26.0,>=0.25.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting watchfiles<0.21.0,>=0.20.0 (from chainlit->-r requirements.txt (line 9))\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting chevron>=0.14.0 (from literalai==0.0.607->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading chevron-0.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community->-r requirements.txt (line 11))\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.11.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from asyncer<0.0.8,>=0.0.7->chainlit->-r requirements.txt (line 9)) (3.7.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json<0.6.0,>=0.5.7->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json<0.6.0,>=0.5.7->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit->-r requirements.txt (line 9)) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit->-r requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit->-r requirements.txt (line 9)) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->chainlit->-r requirements.txt (line 9)) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain->-r requirements.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (2.23.3)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 9)) (0.23.1)\n",
            "Collecting python-engineio>=4.8.0 (from python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading python_engineio-4.9.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers->-r requirements.txt (line 7)) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers->-r requirements.txt (line 7)) (0.19.1)\n",
            "Collecting opentelemetry-api~=1.26 (from uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp~=1.26 (from uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_exporter_otlp-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation~=0.47b0 (from uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-sdk~=1.26 (from uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.4.0->asyncer<0.0.8,>=0.0.7->chainlit->-r requirements.txt (line 9)) (1.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain->-r requirements.txt (line 2)) (3.0.0)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.27.0 (from opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.27.0 (from opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9)) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9)) (1.64.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation~=0.47b0->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9)) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation~=0.47b0->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9)) (1.16.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading simple_websocket-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json<0.6.0,>=0.5.7->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api~=1.26->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 9)) (3.20.1)\n",
            "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 9))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chainlit-1.1.404-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading asyncer-0.0.7-py3-none-any.whl (8.5 kB)\n",
            "Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading Lazify-0.4.0-py2.py3-none-any.whl (3.1 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading python_socketio-5.11.4-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uptrace-1.26.0-py3-none-any.whl (8.6 kB)\n",
            "Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chevron-0.14.0-py3-none-any.whl (11 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.27.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_engineio-4.9.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading simple_websocket-1.0.0-py3-none-any.whl (13 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: literalai, syncer\n",
            "  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for literalai: filename=literalai-0.0.607-py3-none-any.whl size=64141 sha256=e4a2287035fb7d49aeadc57a02c8112f0d7e0dac3cf8e4c9b76fdf9c82ac2737\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/e9/96/826c366cb47a2d2b23265f59beb18f9d5635ab75185cff7a9e\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syncer: filename=syncer-2.0.3-py2.py3-none-any.whl size=3437 sha256=153a6a518d509ef9f0d4a7593d16f8488ec9002c8ea1e55dbaa089cab56c49b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/e4/36/bcaad665bcc2b672888ff2df1a5a1dc638378d8765055313cd\n",
            "Successfully built literalai syncer\n",
            "Installing collected packages: syncer, lazify, filetype, chevron, wsproto, uvicorn, python-multipart, python-dotenv, pypdf, packaging, opentelemetry-proto, mypy-extensions, importlib-metadata, deprecated, aiofiles, watchfiles, typing-inspect, starlette, simple-websocket, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, marshmallow, faiss_cpu, asyncer, python-engineio, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-instrumentation, literalai, fastapi, dataclasses_json, ctransformers, bitsandbytes, python-socketio, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, opentelemetry-exporter-otlp, langchain, uptrace, langchain_community, chainlit\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "Successfully installed aiofiles-23.2.1 asyncer-0.0.7 bitsandbytes-0.43.3 chainlit-1.1.404 chevron-0.14.0 ctransformers-0.2.27 dataclasses_json-0.5.14 deprecated-1.2.14 faiss_cpu-1.8.0.post1 fastapi-0.110.3 filetype-1.2.0 importlib-metadata-8.4.0 langchain-0.3.0 langchain-text-splitters-0.3.0 langchain_community-0.3.0 lazify-0.4.0 literalai-0.0.607 marshmallow-3.22.0 mypy-extensions-1.0.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-exporter-otlp-proto-http-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 packaging-23.2 pydantic-settings-2.5.2 pypdf-4.3.1 python-dotenv-1.0.1 python-engineio-4.9.1 python-multipart-0.0.9 python-socketio-5.11.4 simple-websocket-1.0.0 starlette-0.37.2 syncer-2.0.3 typing-inspect-0.9.0 uptrace-1.26.0 uvicorn-0.25.0 watchfiles-0.20.0 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-huggingface\n",
        "!pip install streamlit\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYixHffBOLmY",
        "outputId": "dd90656f-5242-46b8-c5b1-0e9575780126"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "import chainlit as cl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xIPdN6yHNonN"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/data'\n",
        "DB_FAISS_PATH = '/content/vectorstoredb'\n",
        "\n",
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                   chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "\n",
        "    db = FAISS.from_documents(texts, embeddings)\n",
        "    db.save_local(DB_FAISS_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYkICxI-Q-av"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuFcLEUOR6a0"
      },
      "outputs": [],
      "source": [
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1aNKkGgOU1x"
      },
      "outputs": [],
      "source": [
        "DB_FAISS_PATH = '/content/vectorstoredb'\n",
        "\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "def set_custom_prompt():\n",
        "    \"\"\"\n",
        "    Prompt template for QA retrieval for each vectorstore\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "#Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm, prompt, db):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type='stuff',\n",
        "                                       retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                       )\n",
        "    return qa_chain\n",
        "\n",
        "#Loading the model\n",
        "def load_llm():\n",
        "    # Load the locally downloaded model here\n",
        "      repo_id=\"mistralai/Mistral-7B-v0.1\"\n",
        "      llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=128,temperature=0.1)\n",
        "\n",
        "      return llm\n",
        "\n",
        "#QA Model Function\n",
        "def qa_bot():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
        "    llm = load_llm()\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
        "\n",
        "    return qa\n",
        "\n",
        "#output function\n",
        "def final_result(query):\n",
        "    qa_result = qa_bot()\n",
        "    response = qa_result({'query': query})\n",
        "    return response\n",
        "\n",
        "#chainlit code\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    chain = qa_bot()\n",
        "    msg = cl.Message(content=\"Starting the bot...\")\n",
        "    await msg.send()\n",
        "    msg.content = \"Hi, Welcome to Medical Bot. What is your query?\"\n",
        "    await msg.update()\n",
        "\n",
        "    cl.user_session.set(\"chain\", chain)\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    chain = cl.user_session.get(\"chain\")\n",
        "    cb = cl.AsyncLangchainCallbackHandler(\n",
        "        stream_final_answer=True, answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
        "    )\n",
        "    cb.answer_reached = True\n",
        "    res = await chain.acall(message.content, callbacks=[cb])\n",
        "    answer = res[\"result\"]\n",
        "    sources = res[\"source_documents\"]\n",
        "\n",
        "    if sources:\n",
        "        answer += f\"\\nSources:\" + str(sources)\n",
        "    else:\n",
        "        answer += \"\\nNo sources found\"\n",
        "\n",
        "    await cl.Message(content=answer).send()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxmX-PQzbL2L",
        "outputId": "cae5ec50-b490-4b3e-e69e-5473717e5423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y2Of1yjzSn92",
        "outputId": "409f7f1b-34a2-4cc7-ee57-c125b526a61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit-scrollable-textbox\n",
            "  Downloading streamlit_scrollable_textbox-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.10/dist-packages (from streamlit-scrollable-textbox) (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-scrollable-textbox) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-scrollable-textbox) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-scrollable-textbox) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-scrollable-textbox) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-scrollable-textbox) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-scrollable-textbox) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-scrollable-textbox) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-scrollable-textbox) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-scrollable-textbox) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-scrollable-textbox) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-scrollable-textbox) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-scrollable-textbox) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-scrollable-textbox) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.63->streamlit-scrollable-textbox) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit>=0.63->streamlit-scrollable-textbox) (1.16.0)\n",
            "Downloading streamlit_scrollable_textbox-0.0.3-py3-none-any.whl (971 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m971.3/971.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: streamlit-scrollable-textbox\n",
            "Successfully installed streamlit-scrollable-textbox-0.0.3\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q\n",
        "\n",
        "!pip install --upgrade streamlit -q\n",
        "!pip install streamlit-scrollable-textbox\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dNsEf722S8X",
        "outputId": "fcc4b363-d54d-4a30-b4d9-c061df514177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.81.232.57\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-qkMEykWSrPv",
        "outputId": "730ec1a5-12a9-4955-cb9c-c5ed73d99923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.81.232.57:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://ripe-friends-punch.loca.lt\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "/content/app.py:4: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "2024-09-15 16:05:24.305 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 590, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 134, in <module>\n",
            "    main()\n",
            "  File \"/content/app.py\", line 112, in main\n",
            "    headings = extract_headings_from_pdf(PDF_PATH)\n",
            "  File \"/content/app.py\", line 55, in extract_headings_from_pdf\n",
            "    with open(pdf_path, 'rb') as pdf_file:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/mnt/data/pdf (1).pdf'\n",
            "/content/app.py:4: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb2grXOg0O-X"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "import time\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
        "HF_ACCESS_TOKEN = \"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\"\n",
        "headers = {\"Authorization\": f\"Bearer {HF_ACCESS_TOKEN}\"}\n",
        "\n",
        "DB_FAISS_PATH = '/content/vectorstoredb'\n",
        "SVC_MODEL_PATH = '/content/mlmodel.pkl'\n",
        "\n",
        "# Function to handle API requests with retry mechanism\n",
        "def query(payload):\n",
        "    retries = 3\n",
        "    backoff_factor = 2\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait_time = backoff_factor ** i\n",
        "                st.warning(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries reached. Unable to process the request.\")\n",
        "\n",
        "# Custom LLM function\n",
        "def custom_llm(input_text):\n",
        "    payload = {\n",
        "        \"inputs\": input_text,\n",
        "        \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.1}\n",
        "    }\n",
        "    response = query(payload)\n",
        "\n",
        "    # Check if response is a list and extract the generated text\n",
        "    if isinstance(response, list) and len(response) > 0:\n",
        "        generated_text = response[0].get(\"generated_text\", \"\")\n",
        "    else:\n",
        "        generated_text = \"No response from model.\"\n",
        "\n",
        "    return {\"generated_text\": generated_text}\n",
        "\n",
        "def set_custom_prompt():\n",
        "    custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "    If you don't know the answer don't try to make up an answer.\n",
        "    Briefly describe the disease based on symptoms and suggest treatment options or appropiate doctor .\n",
        "\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "\n",
        "    Only return the helpful answer below and nothing else.\n",
        "    Helpful answer:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "# Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm_func, prompt, db):\n",
        "    def custom_chain(query):\n",
        "        retriever = db.as_retriever(search_kwargs={'k': 2})\n",
        "        documents = retriever.get_relevant_documents(query['query'])\n",
        "        context = \" \".join([doc.page_content for doc in documents])\n",
        "        formatted_prompt = prompt.format(context=context, question=query['query'])\n",
        "\n",
        "        llm_response = llm_func(formatted_prompt)\n",
        "        return {\"result\": llm_response.get(\"generated_text\", \"No response\"), \"source_documents\": documents}\n",
        "\n",
        "    return custom_chain\n",
        "\n",
        "# QA Model Function\n",
        "def qa_bot():\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings,allow_dangerous_deserialization=True)\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa_chain = retrieval_qa_chain(custom_llm, qa_prompt, db)\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "# Streamlit code\n",
        "def main():\n",
        "    st.title(\"DiagnosAI Bot\")\n",
        "\n",
        "    user_query = st.text_input(\"Hi, Welcome to DiagnosAI Bot. What are your symptoms?\")\n",
        "\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if user_query:\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                qa_chain = qa_bot()\n",
        "                res = qa_chain({\"query\": user_query})\n",
        "                answer = res[\"result\"]\n",
        "                sources = res[\"source_documents\"]\n",
        "\n",
        "                st.write(\"*Answer:*\", answer)\n",
        "                if sources:\n",
        "                    st.write(\"*Sources:*\", sources)\n",
        "                else:\n",
        "                    st.write(\"*Sources:* No sources found\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a query.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8FWlUt12M2R"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGtpHLn9ojhH"
      },
      "outputs": [],
      "source": [
        "#Final\n",
        "import streamlit as st\n",
        "import requests\n",
        "import time\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
        "HF_ACCESS_TOKEN = \"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\"\n",
        "headers = {\"Authorization\": f\"Bearer {HF_ACCESS_TOKEN}\"}\n",
        "\n",
        "DB_FAISS_PATH = '/content/vectorstoredb'\n",
        "SVC_MODEL_PATH = '/content/mlmodel.pkl'\n",
        "\n",
        "# Function to handle API requests with retry mechanism\n",
        "def query(payload):\n",
        "    retries = 3\n",
        "    backoff_factor = 2\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait_time = backoff_factor ** i\n",
        "                st.warning(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries reached. Unable to process the request.\")\n",
        "\n",
        "# Custom LLM function\n",
        "def custom_llm(input_text):\n",
        "    payload = {\n",
        "        \"inputs\": input_text,\n",
        "        \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.1}\n",
        "    }\n",
        "    response = query(payload)\n",
        "\n",
        "    # Check if response is a list and extract the generated text\n",
        "    if isinstance(response, list) and len(response) > 0:\n",
        "        generated_text = response[0].get(\"generated_text\", \"\")\n",
        "    else:\n",
        "        generated_text = \"No response from model.\"\n",
        "\n",
        "    return {\"generated_text\": generated_text}\n",
        "\n",
        "def set_custom_prompt():\n",
        "    custom_prompt_template = \"\"\"Use the following context to answer the user's question about medical conditions. If the answer cannot be found in the context, state that you don't know.\n",
        "\n",
        "      Context: Information about various medical conditions, including their short descriptions, symptoms, precautions, treatments, and types of doctors to consult.\n",
        "\n",
        "      Question: {question}\n",
        "\n",
        "      Provide the most relevant information based on the context. If the answer is not available, respond with \"I don't know.\"\n",
        "      Helpful answer:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "# Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm_func, prompt, db):\n",
        "    def custom_chain(query):\n",
        "        retriever = db.as_retriever(search_kwargs={'k': 2})\n",
        "        documents = retriever.get_relevant_documents(query['query'])\n",
        "        context = \" \".join([doc.page_content for doc in documents])\n",
        "        formatted_prompt = prompt.format(context=context, question=query['query'])\n",
        "\n",
        "        llm_response = llm_func(formatted_prompt)\n",
        "        return {\"result\": llm_response.get(\"generated_text\", \"No response\"), \"source_documents\": documents}\n",
        "\n",
        "    return custom_chain\n",
        "\n",
        "# QA Model Function\n",
        "def qa_bot():\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa_chain = retrieval_qa_chain(custom_llm, qa_prompt, db)\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "# Streamlit code\n",
        "def main():\n",
        "    st.title(\"DiagnosAI Bot\")\n",
        "\n",
        "    # Initialize session state variables\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"end_chat\" not in st.session_state:\n",
        "        st.session_state.end_chat = False\n",
        "\n",
        "    # User input and button to get the answer\n",
        "    user_query = st.text_input(\"What are your symptoms?\", key=\"user_query\")\n",
        "\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if user_query:\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                qa_chain = qa_bot()\n",
        "                res = qa_chain({\"query\": user_query})\n",
        "                answer = res[\"result\"]\n",
        "                sources = res[\"source_documents\"]\n",
        "\n",
        "                # Update chat history in session state\n",
        "                st.session_state.chat_history.append(f\"You: {user_query}\")\n",
        "                st.session_state.chat_history.append(f\"Bot: {answer}\")\n",
        "\n",
        "                # Display chat history\n",
        "                for message in st.session_state.chat_history:\n",
        "                    st.write(message)\n",
        "\n",
        "                # Optionally display sources:\n",
        "                # if sources:\n",
        "                #     st.write(\"*Sources:*\")\n",
        "                #     for doc in sources:\n",
        "                #         st.write(doc)\n",
        "                # else:\n",
        "                #     st.write(\"*Sources:* No sources found\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a query.\")\n",
        "\n",
        "    # End Chat button\n",
        "    if st.button(\"End Chat\"):\n",
        "        st.session_state.end_chat = True\n",
        "        st.write(\"Chat ended.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T7VDZm-golcq",
        "outputId": "fb7cf8e0-a636-4bb2-c3da-c7b019ba4337"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-28 17:48:55.866 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.300 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-08-28 17:48:56.303 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.306 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.309 Session state does not function when running a script without `streamlit run`\n",
            "2024-08-28 17:48:56.314 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.315 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.323 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.326 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.329 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.333 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.337 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.341 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.343 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.357 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.358 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.359 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.361 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.362 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-08-28 17:48:56.363 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "#finall 2\n",
        "import streamlit as st\n",
        "import requests\n",
        "import time\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
        "HF_ACCESS_TOKEN = \"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\"\n",
        "headers = {\"Authorization\": f\"Bearer {HF_ACCESS_TOKEN}\"}\n",
        "\n",
        "DB_FAISS_PATH = '/content/vectorstoredb'\n",
        "SVC_MODEL_PATH = '/content/mlmodel.pkl'\n",
        "\n",
        "# Function to handle API requests with retry mechanism\n",
        "def query(payload):\n",
        "    retries = 3\n",
        "    backoff_factor = 2\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait_time = backoff_factor ** i\n",
        "                st.warning(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries reached. Unable to process the request.\")\n",
        "\n",
        "# Custom LLM function\n",
        "def custom_llm(input_text):\n",
        "    payload = {\n",
        "        \"inputs\": input_text,\n",
        "        \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.1}\n",
        "    }\n",
        "    response = query(payload)\n",
        "\n",
        "    # Check if response is a list and extract the generated text\n",
        "    if isinstance(response, list) and len(response) > 0:\n",
        "        generated_text = response[0].get(\"generated_text\", \"\")\n",
        "    else:\n",
        "        generated_text = \"No response from model.\"\n",
        "\n",
        "    return {\"generated_text\": generated_text}\n",
        "\n",
        "def set_custom_prompt():\n",
        "    custom_prompt_template = \"\"\"Use the following context to answer the user's question about medical conditions. If the answer cannot be found in the context, state that you don't know.\n",
        "\n",
        "      Context: Information about various medical conditions, including their short descriptions, symptoms, precautions, treatments, and types of doctors to consult.\n",
        "\n",
        "      Question: {question}\n",
        "\n",
        "      Provide the most relevant information based on the context. If the answer is not available, respond with \"I don't know.\"\n",
        "      Helpful answer:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "# Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm_func, prompt, db):\n",
        "    def custom_chain(query):\n",
        "        retriever = db.as_retriever(search_kwargs={'k': 2})\n",
        "        documents = retriever.get_relevant_documents(query['query'])\n",
        "        context = \" \".join([doc.page_content for doc in documents])\n",
        "        formatted_prompt = prompt.format(context=context, question=query['query'])\n",
        "\n",
        "        llm_response = llm_func(formatted_prompt)\n",
        "        return {\"result\": llm_response.get(\"generated_text\", \"No response\"), \"source_documents\": documents}\n",
        "\n",
        "    return custom_chain\n",
        "\n",
        "# QA Model Function\n",
        "def qa_bot():\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa_chain = retrieval_qa_chain(custom_llm, qa_prompt, db)\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "# Streamlit code\n",
        "def main():\n",
        "    st.title(\"DiagnosAI Bot\")\n",
        "\n",
        "    # Initialize session state variables\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"end_chat\" not in st.session_state:\n",
        "        st.session_state.end_chat = False\n",
        "\n",
        "    # Display chat history\n",
        "    st.write(\"### Chat History\")\n",
        "    for message in st.session_state.chat_history:\n",
        "        st.write(message)\n",
        "\n",
        "    st.write(\"---\")  # A horizontal line to separate the chat history from the input section\n",
        "\n",
        "    # User input section\n",
        "    with st.form(key='user_input_form', clear_on_submit=True):\n",
        "        user_query = st.text_input(\"What are your symptoms?\", key=\"user_query\")\n",
        "        submit_button = st.form_submit_button(label='Get Answer')\n",
        "\n",
        "        if submit_button and user_query:\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                qa_chain = qa_bot()\n",
        "                res = qa_chain({\"query\": user_query})\n",
        "                answer = res[\"result\"]\n",
        "\n",
        "                # Update chat history in session state\n",
        "                st.session_state.chat_history.append(f\"You: {user_query}\")\n",
        "                st.session_state.chat_history.append(f\"Bot: {answer}\")\n",
        "\n",
        "    # End Chat button\n",
        "    if st.button(\"End Chat\"):\n",
        "        st.session_state.end_chat = True\n",
        "        st.write(\"Chat ended.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFFjo7WQUPQi"
      },
      "outputs": [],
      "source": [
        "# Final 3\n",
        "import streamlit as st\n",
        "import requests\n",
        "import time\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import streamlit_scrollable_textbox as stx\n",
        "\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
        "HF_ACCESS_TOKEN = \"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\"\n",
        "headers = {\"Authorization\": f\"Bearer {HF_ACCESS_TOKEN}\"}\n",
        "\n",
        "DB_FAISS_PATH = '/content/vectorstoredb'\n",
        "SVC_MODEL_PATH = '/content/mlmodel.pkl'\n",
        "\n",
        "# Function to handle API requests with retry mechanism\n",
        "def query(payload):\n",
        "    retries = 3\n",
        "    backoff_factor = 2\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait_time = backoff_factor ** i\n",
        "                st.warning(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries reached. Unable to process the request.\")\n",
        "\n",
        "def load_svc_model():\n",
        "    svc_model = joblib.load(SVC_MODEL_PATH)\n",
        "    return svc_model\n",
        "\n",
        "def extract_symptoms(user_query):\n",
        "    symptoms = user_query.split(\", \")\n",
        "    return symptoms\n",
        "\n",
        "def predict_disease(symptoms, svc_model):\n",
        "    prediction = svc_model.predict([symptoms])[0]\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# Custom LLM function\n",
        "def custom_llm(input_text):\n",
        "    payload = {\n",
        "        \"inputs\": input_text,\n",
        "        \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.1}\n",
        "    }\n",
        "    response = query(payload)\n",
        "\n",
        "    # Check if response is a list and extract the generated text\n",
        "    if isinstance(response, list) and len(response) > 0:\n",
        "        generated_text = response[0].get(\"generated_text\", \"\")\n",
        "    else:\n",
        "        generated_text = \"No response from model.\"\n",
        "\n",
        "    return {\"generated_text\": generated_text}\n",
        "\n",
        "def set_custom_prompt():\n",
        "    custom_prompt_template = \"\"\"Use the following context to answer the user's question about medical conditions. If the answer cannot be found in the context, state that you don't know.\n",
        "\n",
        "      Context: Information about various medical conditions, including their short descriptions, symptoms, precautions, treatments, and types of doctors to consult.\n",
        "\n",
        "      Question: {question}\n",
        "\n",
        "      Provide the most relevant information based on the context. If the answer is not available, respond with \"I don't know.\"\n",
        "      Helpful answer:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "# Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm_func, prompt, db):\n",
        "    def custom_chain(query):\n",
        "        retriever = db.as_retriever(search_kwargs={'k': 2})\n",
        "        documents = retriever.get_relevant_documents(query['query'])\n",
        "        context = \" \".join([doc.page_content for doc in documents])\n",
        "        formatted_prompt = prompt.format(context=context, question=query['query'])\n",
        "\n",
        "        llm_response = llm_func(formatted_prompt)\n",
        "        return {\"result\": llm_response.get(\"generated_text\", \"No response\"), \"source_documents\": documents}\n",
        "\n",
        "    return custom_chain\n",
        "\n",
        "\n",
        "# QA Model Function\n",
        "def qa_bot(disease):\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa_chain = retrieval_qa_chain(custom_llm, qa_prompt, db)\n",
        "\n",
        "    return qa_chain\n",
        "# Output function\n",
        "def final_result(query, svc_model):\n",
        "    symptoms = extract_symptoms(query)\n",
        "    predicted_disease = predict_disease(symptoms, svc_model)\n",
        "    qa_result = qa_bot(predicted_disease)\n",
        "    return qa_result\n",
        "# Streamlit code\n",
        "import streamlit as st\n",
        "\n",
        "def main():\n",
        "    st.title(\"DiagnosAI Bot\")\n",
        "\n",
        "    # Initialize session state variables\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"end_chat\" not in st.session_state:\n",
        "        st.session_state.end_chat = False\n",
        "\n",
        "    # Display previous messages\n",
        "    for message in st.session_state.chat_history:\n",
        "        role, text = message.split(\": \", 1)\n",
        "        with st.chat_message(role.lower()):\n",
        "            st.markdown(text)\n",
        "\n",
        "    st.write(\"---\")  # A horizontal line to separate the chat history from the input section\n",
        "\n",
        "    # User input section\n",
        "    with st.form(key='user_input_form', clear_on_submit=True):\n",
        "        user_query = st.text_input(\"What are your symptoms?\", key=\"user_query\")\n",
        "        submit_button = st.form_submit_button(label='Get Answer')\n",
        "\n",
        "        if submit_button and user_query:\n",
        "            # Display user query\n",
        "            with st.chat_message(\"user\"):\n",
        "                st.markdown(user_query)\n",
        "            st.session_state.chat_history.append(f\"You: {user_query}\")\n",
        "\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                qa_chain = qa_bot()\n",
        "                res = qa_chain({\"query\": user_query})\n",
        "                answer = res[\"result\"]\n",
        "\n",
        "                # Display the assistant's response\n",
        "                with st.chat_message(\"bot\"):\n",
        "                    message_placeholder = st.empty()\n",
        "                    message_placeholder.markdown(answer + \"▌\")\n",
        "                    message_placeholder.markdown(answer)\n",
        "\n",
        "                # Update chat history in session state\n",
        "                st.session_state.chat_history.append(f\"Bot: {answer}\")\n",
        "\n",
        "    # End Chat button\n",
        "    if st.button(\"End Chat\"):\n",
        "        st.session_state.end_chat = True\n",
        "        st.write(\"Chat ended.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHP-qDHEDRiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}